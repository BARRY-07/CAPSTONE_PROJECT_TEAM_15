{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAPSTONE PROJECT 2023-2024\n",
    "### Financial Graph Mining For Customers & Supply Chains Assessment\n",
    "In order to improve the Bank’s CIB activities, a special attention is given to its clients transactions. Mapping out the supply chain of corporate customers is a strategic move that can enhance the bank’s risk management, client relationship and competitiveness in the market."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import networkx as nx\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import from_networkx\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import networkx as nx\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import BatchNorm\n",
    "from torch.nn import Linear\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  PART 1: EXPLORATORY DATA ANALYSIS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Importing datasets and treatment of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_data = pd.read_csv('/home/jovyan/hfactory_magic_folders/financial_graph_mining_for_customers___supply_chains_assessment/static_data_all_x.csv',sep=';')\n",
    "transactions_data = pd.read_csv('/home/jovyan/hfactory_magic_folders/financial_graph_mining_for_customers___supply_chains_assessment/transactions_x.csv',sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_probability(value):\n",
    "    if '+' in str(value):\n",
    "        return float(value.replace('+', '')) + 0.25\n",
    "    elif '-' in str(value):\n",
    "        return float(value.replace('-', '')) - 0.25\n",
    "    else:\n",
    "        return float(value)\n",
    "def from_pred_to_normal(number):\n",
    "\n",
    "    if 0 <= number <= 13:\n",
    "        if number == 13:\n",
    "            return \"-13\"\n",
    "        else:\n",
    "            base = int(number)\n",
    "            if number - base < 0.1:\n",
    "                return f\"{base}\"\n",
    "            else:\n",
    "                if number - base < 0.4:\n",
    "                    return f\"{base}+\"\n",
    "                else:\n",
    "                    return f\"-{base+1}\"\n",
    "\n",
    "static_data['T_LOCAL_TX_PD'] = static_data['T_LOCAL_TX_PD'].apply(convert_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_=pd.read_csv('/home/jovyan/hfactory_magic_folders/financial_graph_mining_for_customers___supply_chains_assessment/static_data_x_PD.csv',sep=';')\n",
    "\n",
    "## filtre solon T_LOCAL_MT_ACTIF_SOCIAL=NAN et QUARTER=\"q4\"\n",
    "test_data = test_data_.loc[(pd.isna(test_data_['T_LOCAL_MT_ACTIF_SOCIAL'])) & (test_data_['QUARTER'] == \"q4\")]\n",
    "test_data_ = test_data_.loc[(pd.isna(test_data_['T_LOCAL_MT_ACTIF_SOCIAL'])) & (test_data_['QUARTER'] == \"q4\")]\n",
    "test_data['T_LOCAL_TX_PD'] = test_data['T_LOCAL_TX_PD'].apply(convert_probability)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAITEMENT df,df_train ET df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=static_data\n",
    "df_merged = pd.merge(test_data, df[['ID', 'T_LOCAL_MT_ACTIF_SOCIAL']], on='ID', how='left', suffixes=('', '_from_static_data'))\n",
    "df_merged=df_merged.drop_duplicates()\n",
    "df_merged.drop(columns=['T_LOCAL_MT_ACTIF_SOCIAL'], inplace=True)\n",
    "df_merged.rename(columns={'T_LOCAL_MT_ACTIF_SOCIAL_from_static_data': 'T_LOCAL_MT_ACTIF_SOCIAL'}, inplace=True)\n",
    "df_test=df_merged\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.merge(df, df_test[['ID', 'QUARTER']], on=['ID', 'QUARTER'], how='left', indicator=True)\n",
    "df_train = df_train[df_train['_merge'] == 'left_only']\n",
    "df_train.drop(columns=['_merge'], inplace=True)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehotencoding(df,columns=None):\n",
    "    # One-hot encode 'CATEGORY','REGION' and 'QUARTER'\n",
    "    category_one_hot = pd.get_dummies(df['CATEGORY'], prefix='Cat')\n",
    "    region_one_hot = pd.get_dummies(df['REGION'], prefix='Reg')\n",
    "    quarter_one_hot=pd.get_dummies(df['QUARTER'])\n",
    "\n",
    "    df = pd.concat([df, category_one_hot, region_one_hot,quarter_one_hot], axis=1)\n",
    "\n",
    "    df.drop(['CATEGORY', 'REGION','QUARTER'], axis=1, inplace=True)\n",
    "    df_converted = df.copy()\n",
    "    if columns is None:\n",
    "        columns = df_converted.columns\n",
    "    for col in columns:\n",
    "        if df_converted[col].dtype == bool:\n",
    "            df_converted[col] = df_converted[col].astype(int)\n",
    "    return df_converted\n",
    "df_test_encoded=onehotencoding(df_test)\n",
    "df_train_encoded=onehotencoding(df_train)\n",
    "df_test_encoded = df_test_encoded.reindex(columns=df_train_encoded.columns, fill_value=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Basics analysis on features (we will use static data and transactions data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will do some analysis on the companies only and make analysis based on quarter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_data_unique=static_data.drop_duplicates(subset='ID') # To delete duplicates companies based on ID\n",
    "print(static_data_unique.describe())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In total we have 1129 companies in our dataset. Default probability accross them is around 5.6 (from 0 to 13)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_counts = static_data_unique['CATEGORY'].value_counts()\n",
    "region_counts = static_data_unique['REGION'].value_counts()\n",
    "total_categories = categories_counts.sum()\n",
    "percentages_categories = 100 * categories_counts / total_categories\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Catégories\n",
    "sns.barplot(x=categories_counts.index, y=categories_counts.values, ax=ax1)\n",
    "ax1.set_title('Distribution of Companies Across Categories')\n",
    "ax1.set_xlabel('Category')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Ajout des étiquettes de pourcentage sur l'axe des y pour le graphique des catégories\n",
    "ax1.set_yticklabels([f'{p:.2f}%' for p in np.linspace(0, 60, len(ax1.get_yticks()))])\n",
    "\n",
    "# Régions\n",
    "ax2.pie(region_counts, labels=region_counts.index, autopct='%1.1f%%', startangle=140)\n",
    "ax2.set_title('Distribution of Companies Across Regions')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More than 55% of the companies are in raw materials in our database. Companies in OEM are the most less represented (less than 10%).\n",
    "We also have around 60% of companies in the region APAC. The region EMEA follows with exactly 15.8% of the companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_by_quarter = static_data.groupby('QUARTER')['T_LOCAL_TX_PD'].mean().reset_index()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(x='QUARTER', y='T_LOCAL_TX_PD', data=mean_by_quarter, marker='o')\n",
    "plt.title('Evolution of the Mean of T_LOCAL_TX_PD by Quarter')\n",
    "plt.xlabel('Qarter')\n",
    "plt.ylabel('Mean of T_LOCAL_TX_PD')\n",
    "plt.xticks(rotation=45) \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our target variable mean  was quit high according to the other quarter in Q2. This mean was lesser in Q4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_by_quarter_ = static_data.groupby('QUARTER')['ESG_SCORE'].mean().reset_index()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(x='QUARTER', y='ESG_SCORE', data=mean_by_quarter_, marker='o')\n",
    "plt.title('Evolution of the Mean of ESG_SCORE by Quarter')\n",
    "plt.xlabel('Qarter')\n",
    "plt.ylabel('Mean of ESG_SCORE')\n",
    "plt.xticks(rotation=45) \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During all quarters the mean of the ESG score stayed at the same level which is around 3.87."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_by_cat = static_data.groupby('CATEGORY')['T_LOCAL_TX_PD'].mean().reset_index()\n",
    "mean_by_reg = static_data.groupby('REGION')['T_LOCAL_TX_PD'].mean().reset_index()\n",
    "\n",
    "mean_by_cat['Type'] = 'Category'\n",
    "mean_by_reg['Type'] = 'Region'\n",
    "mean_by_cat.rename(columns={'CATEGORY': 'Group'}, inplace=True)\n",
    "mean_by_reg.rename(columns={'REGION': 'Group'}, inplace=True)\n",
    "\n",
    "combined_data = pd.concat([mean_by_cat, mean_by_reg])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Group', y='T_LOCAL_TX_PD', hue='Type', data=combined_data, dodge=False)\n",
    "plt.title('Mean of T_LOCAL_TX_PD by Category and Region')\n",
    "plt.xlabel('Groups')\n",
    "plt.ylabel('Mean of T_LOCAL_TX_PD')\n",
    "plt.xticks(rotation=45)\n",
    "min_val = combined_data['T_LOCAL_TX_PD'].min() * 0.95 \n",
    "max_val = combined_data['T_LOCAL_TX_PD'].max() * 1.05\n",
    "plt.ylim(min_val, max_val)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Companies in dealers category had a higher PD mean than other categories. The following category is company in tier2 and tier1.\n",
    "We also notice that companies in the region EMEA have the biggest PD mean accross this period.\n",
    "The region and category can be a little bit relevant to help predicting PD even if their contributions can be be small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(static_data[['ESG_SCORE','T_LOCAL_MT_ACTIF_SOCIAL','T_LOCAL_TX_PD']].corr(), annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix of Features')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a positive correlation between T_LOCAL_MT_ACTIF_SOCIAL and T_LOCAL_TX_PD but this one is very small (0.012)\n",
    "We also have a very small negative correlation between our target and T_LOCAL_MT_ACTIF_SOCIAL(-0.018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_train = nx.DiGraph()\n",
    "\n",
    "for _, row in df_train_encoded.iterrows():\n",
    "    node_features = row.drop(['ID', 'T_LOCAL_TX_PD']).to_dict()  \n",
    "    G_train.add_node(row['ID'], **node_features, t_local_tx_pd=row['T_LOCAL_TX_PD'])\n",
    "\n",
    "\n",
    "for _, row in transactions_data.iterrows():\n",
    "    if row['ID'] in G_train and row['COUNTERPARTY'] in G_train:  \n",
    "        G_train.add_edge(row['ID'], row['COUNTERPARTY'], date=row['DATE'], tx_amount=row['TX_AMOUNT'])\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Train Graph has {len(G_train.nodes)} nodes and {len(G_train.edges)} edges.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_test = nx.DiGraph()\n",
    "\n",
    "for _, row in df_test_encoded.iterrows():\n",
    "    node_features = row.drop(['ID', 'T_LOCAL_TX_PD']).to_dict()  \n",
    "    G_test.add_node(row['ID'], **node_features, t_local_tx_pd=row['T_LOCAL_TX_PD'])\n",
    "\n",
    "\n",
    "for _, row in transactions_data.iterrows():\n",
    "    if row['ID'] in G_test and row['COUNTERPARTY'] in G_test:  \n",
    "        G_test.add_edge(row['ID'], row['COUNTERPARTY'], date=row['DATE'], tx_amount=row['TX_AMOUNT'])\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Test Graph has {len(G_test.nodes)} nodes and {len(G_test.edges)} edges.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_neighborhood(depth,G=G_train, node_id=373869):\n",
    "\n",
    "    neighbors = nx.single_source_shortest_path_length(G, node_id, cutoff=depth).keys()\n",
    "    subgraph = G.subgraph(neighbors)\n",
    "    pos = nx.spring_layout(subgraph, seed=42)\n",
    "\n",
    "\n",
    "    edge_x, edge_y = [], []\n",
    "    for edge in subgraph.edges():\n",
    "        x0, y0 = pos[edge[0]]\n",
    "        x1, y1 = pos[edge[1]]\n",
    "        edge_x.extend([x0, x1, None])\n",
    "        edge_y.extend([y0, y1, None])\n",
    "\n",
    "    edge_trace = go.Scatter(x=edge_x, y=edge_y, line=dict(width=2, color='#888'),\n",
    "                            hoverinfo='none', mode='lines')\n",
    "\n",
    "    node_x, node_y = [], []\n",
    "    node_colors = []\n",
    "    depth_colors = ['red', 'green', 'blue', 'purple', 'orange', 'yellow', 'pink', 'brown', 'grey', 'cyan']  \n",
    "\n",
    "    for node in subgraph.nodes():\n",
    "        x, y = pos[node]\n",
    "        node_x.append(x)\n",
    "        node_y.append(y)\n",
    "        node_depth = nx.shortest_path_length(G, source=node_id, target=node)\n",
    "        color = depth_colors[node_depth] if node_depth < len(depth_colors) else 'black'\n",
    "        node_colors.append(color)\n",
    "\n",
    "    node_trace = go.Scatter(x=node_x, y=node_y, mode='markers', hoverinfo='text',\n",
    "                            marker=dict(showscale=False, colorscale='Viridis', size=10, color=node_colors, line_width=2))\n",
    "\n",
    " \n",
    "    node_trace.text = [f'ID: {node}' for node in subgraph.nodes()]\n",
    "\n",
    "\n",
    "    fig = go.Figure(data=[edge_trace, node_trace], layout=go.Layout(\n",
    "                title=f'{depth}-Deep Neighborhood of Node {node_id}',\n",
    "                titlefont_size=16,\n",
    "                showlegend=False,\n",
    "                hovermode='closest',\n",
    "                margin=dict(b=20,l=5,r=5,t=40),\n",
    "                xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)))\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "for i in [1,2,3]:\n",
    "    plot_neighborhood(i)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2: MODELING"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let try some simple models on our train set without using the transactions informations to see how they performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=df_train_encoded.drop(columns=[\"T_LOCAL_TX_PD\",\"ID\"])\n",
    "X_test=df_test_encoded.drop(columns=[\"T_LOCAL_TX_PD\",\"ID\"])\n",
    "y_train=df_train_encoded[[\"T_LOCAL_TX_PD\"]]\n",
    "y_test=df_test_encoded[[\"T_LOCAL_TX_PD\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1: Using Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100], \n",
    "    'max_depth': [None, 10, 20, 30], \n",
    "    'min_samples_split': [2, 4, 6]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=RandomForestRegressor(random_state=42),\n",
    "                           param_grid=param_grid,\n",
    "                           cv=5,  \n",
    "                           scoring='neg_mean_squared_error',  \n",
    "                           verbose=1,  \n",
    "                           n_jobs=-1)\n",
    "\n",
    "\n",
    "grid_search.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "y_train_pred = best_model.predict(X_train)\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Training R2 score: {train_r2:.2f}\")\n",
    "print(f\"Test R2 score: {test_r2:.2f}\")\n",
    "print(f\"Training MSE: {train_mse:.2f}\")\n",
    "print(f\"Test MSE: {test_mse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_['PD random forest'] = np.array([from_pred_to_normal(y) for y in y_test_pred])\n",
    "test_data_[[\"T_LOCAL_TX_PD\",'PD random forest']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2: Let try GNN for predicting PD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_data_train = from_networkx(G_train)\n",
    "G_data_train.x = torch.tensor([list(data.values()) for _, data in G_train.nodes(data=True)], dtype=torch.float)\n",
    "G_data_train.y = torch.tensor([data['t_local_tx_pd'] for _, data in G_train.nodes(data=True)], dtype=torch.float)\n",
    "\n",
    "\n",
    "G_data_test = from_networkx(G_test)\n",
    "G_data_test.x = torch.tensor([list(data.values()) for _, data in G_test.nodes(data=True)], dtype=torch.float)\n",
    "G_data_test.y = torch.tensor([data['t_local_tx_pd'] for _, data in G_test.nodes(data=True)], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedGCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, hidden_channels, output_channels=1, dropout_rate=0.5):\n",
    "        super(EnhancedGCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)  \n",
    "        self.batch_norm1 = BatchNorm(hidden_channels)  \n",
    "        self.batch_norm2 = BatchNorm(hidden_channels) \n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.out = torch.nn.Linear(hidden_channels, output_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # 1st layer\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.batch_norm1(x)\n",
    "        x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "\n",
    "        # 2st  layer\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.batch_norm2(x)\n",
    "        x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "\n",
    "        # 3rd  layer\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "\n",
    "        \n",
    "        x = self.out(x)\n",
    "        return x\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_node_features = G_data_train.num_node_features\n",
    "hidden_channels = 64  \n",
    "output_channels = 1  \n",
    "dropout_rate = 0.5 \n",
    "\n",
    "model = EnhancedGCN(num_node_features, hidden_channels, output_channels, dropout_rate)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  \n",
    "loss_func = torch.nn.MSELoss() \n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(G_data_train.x, G_data_train.edge_index)\n",
    "    loss = loss_func(out.squeeze(), G_data_train.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "for epoch in range(2000):\n",
    "    loss = train()\n",
    "    print(f'Epoch {epoch+1}: Loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index)\n",
    "        loss = loss_func(out.squeeze(), data.y)\n",
    "    return loss.item()\n",
    "\n",
    "test_loss = evaluate(G_data_test)\n",
    "print(f'Test Loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(data.x, data.edge_index)\n",
    "    return predictions.squeeze()\n",
    "\n",
    "predictions = predict(G_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_['PD GNN'] = np.array([from_pred_to_normal(y) for y in predictions])\n",
    "test_data_[[\"T_LOCAL_TX_PD\",'PD random forest','PD GNN']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2: LET RESCALE NUMERICAL VARIABLES AND RETRY GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "num_features_static=[\"ESG_SCORE\",\"T_LOCAL_MT_ACTIF_SOCIAL\"]\n",
    "num_features_transac=[\"TX_AMOUNT\"]\n",
    "\n",
    "df_test_encoded[num_features_static] = scaler.fit_transform(df_test_encoded[num_features_static])\n",
    "df_train_encoded[num_features_static] = scaler.fit_transform(df_train_encoded[num_features_static])\n",
    "transactions_data[num_features_transac]=scaler.fit_transform(transactions_data[num_features_transac])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_train = nx.DiGraph()\n",
    "\n",
    "for _, row in df_train_encoded.iterrows():\n",
    "    node_features = row.drop(['ID', 'T_LOCAL_TX_PD']).to_dict()  \n",
    "    G_train.add_node(row['ID'], **node_features, t_local_tx_pd=row['T_LOCAL_TX_PD'])\n",
    "\n",
    "\n",
    "for _, row in transactions_data.iterrows():\n",
    "    if row['ID'] in G_train and row['COUNTERPARTY'] in G_train:  \n",
    "        G_train.add_edge(row['ID'], row['COUNTERPARTY'], date=row['DATE'], tx_amount=row['TX_AMOUNT'])\n",
    "\n",
    "print(f\"Train Graph has {len(G_train.nodes)} nodes and {len(G_train.edges)} edges.\")\n",
    "\n",
    "G_test = nx.DiGraph()\n",
    "\n",
    "for _, row in df_test_encoded.iterrows():\n",
    "    node_features = row.drop(['ID', 'T_LOCAL_TX_PD']).to_dict()  \n",
    "    G_test.add_node(row['ID'], **node_features, t_local_tx_pd=row['T_LOCAL_TX_PD'])\n",
    "\n",
    "\n",
    "for _, row in transactions_data.iterrows():\n",
    "    if row['ID'] in G_test and row['COUNTERPARTY'] in G_test:  \n",
    "        G_test.add_edge(row['ID'], row['COUNTERPARTY'], date=row['DATE'], tx_amount=row['TX_AMOUNT'])\n",
    "\n",
    "print(f\"Test Graph has {len(G_test.nodes)} nodes and {len(G_test.edges)} edges.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_data_train = from_networkx(G_train)\n",
    "G_data_train.x = torch.tensor([list(data.values()) for _, data in G_train.nodes(data=True)], dtype=torch.float)\n",
    "G_data_train.y = torch.tensor([data['t_local_tx_pd'] for _, data in G_train.nodes(data=True)], dtype=torch.float)\n",
    "\n",
    "G_data_test = from_networkx(G_test)\n",
    "G_data_test.x = torch.tensor([list(data.values()) for _, data in G_test.nodes(data=True)], dtype=torch.float)\n",
    "G_data_test.y = torch.tensor([data['t_local_tx_pd'] for _, data in G_test.nodes(data=True)], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedGCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, hidden_channels, output_channels=1, dropout_rate=0.5):\n",
    "        super(EnhancedGCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)  \n",
    "        self.batch_norm1 = BatchNorm(hidden_channels)  \n",
    "        self.batch_norm2 = BatchNorm(hidden_channels) \n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.out = torch.nn.Linear(hidden_channels, output_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # 1st layer\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.batch_norm1(x)\n",
    "        x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "\n",
    "        # 2st  layer\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.batch_norm2(x)\n",
    "        x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "\n",
    "        # 3rd  layer\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "\n",
    "        \n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_node_features = G_data_train.num_node_features\n",
    "hidden_channels = 70  \n",
    "output_channels = 1  \n",
    "dropout_rate = 0.2 \n",
    "\n",
    "model = EnhancedGCN(num_node_features, hidden_channels, output_channels, dropout_rate)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  \n",
    "loss_func = torch.nn.MSELoss() \n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(G_data_train.x, G_data_train.edge_index)\n",
    "    loss = loss_func(out.squeeze(), G_data_train.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "for epoch in range(2000):\n",
    "    loss = train()\n",
    "    print(f'Epoch {epoch+1}: Loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index)\n",
    "        loss = loss_func(out.squeeze(), data.y)\n",
    "    return loss.item()\n",
    "\n",
    "test_loss = evaluate(G_data_test)\n",
    "print(f'Test Loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(data.x, data.edge_index)\n",
    "    return predictions.squeeze()\n",
    "\n",
    "predictions = predict(G_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_['PD GNN 2'] = np.array([from_pred_to_normal(y) for y in predictions])\n",
    "test_data_[[\"T_LOCAL_TX_PD\",'PD random forest','PD GNN','PD GNN 2']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 3: ANALYSIS AND CONCLUSIONS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest is the best estimator we can find for predicting PD in q4. Here the GNN predict mostly values between 4 and 5. This is less accurate even if this model take in account transactions.\n",
    "\n",
    "After scaling the new GNN is more better than the previous GNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
